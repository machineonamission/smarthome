[Unit]
Description=Ollama Brain

[Container]
Pod=smarthome.pod
ContainerName=ollama
Image=ollama_fork.build
Pull=missing
Volume=ollama-data.volume:/root/.ollama:Z
AddDevice=nvidia.com/gpu=all
Environment=OLLAMA_DEBUG=2
Environment=OLLAMA_FLASH_ATTENTION=1
Environment=OLLAMA_KV_CACHE_TYPE=q4_0
Environment=OLLAMA_CONTEXT_LENGTH=8000
Environment=OLLAMA_VULKAN=1
Environment=OLLAMA_MAX_VRAM=2400000000
Environment=OLLAMA_KEEP_ALIVE=-1

HealthCmd=out=$(curl -s http://localhost:11434/); echo "$out"; echo "$out" | grep -q "Ollama is running" || exit 1
HealthInterval=10s
HealthTimeout=10s
HealthRetries=10
Notify=healthy

[Service]
Restart=always
ExecStartPost=-/usr/sbin/curl http://localhost:11434/api/pull -d '{"model": "qwen3:4b-instruct"}'
ExecStartPost=-/usr/sbin/curl -s -X POST http://localhost:11434/api/generate -d '{"model":"qwen3:4b-instruct","keep_alive":-1}'
